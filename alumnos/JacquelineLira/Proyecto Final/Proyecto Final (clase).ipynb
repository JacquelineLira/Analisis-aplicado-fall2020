{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random\n",
    "\n",
    "class Optimizador:\n",
    "    def __init__(self, f, x0, max_iter = 10000, tol = 0.001):\n",
    "        self.f = f\n",
    "        self.x0 = x0\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "    \n",
    "    # Función que calcula la gradiente de f en el punto xk\n",
    "    def gradiente(self,xk, eps = 1e-6):\n",
    "        size = len(xk)\n",
    "        grad = np.zeros(size, dtype = float)\n",
    "        for i in range(size):\n",
    "            x_f = xk.copy(); \n",
    "            x_f[i] += eps\n",
    "            x_b = xk.copy(); x_b[i] -= eps\n",
    "            grad[i] = (self.f(x_f) - self.f(x_b)) / (2 * eps)\n",
    "        return grad    \n",
    "\n",
    "    # Función que calcula la hessiana de f en el punto xk\n",
    "    def hessiana(self, xk, eps = 1e-3):\n",
    "        size = len(xk)\n",
    "        hess = np.zeros((size, size))\n",
    "        for j in range(size):\n",
    "            # Primera derivada\n",
    "            dx_ff = xk.copy(); dx_ff[j] += eps;\n",
    "            dx_bb = xk.copy(); dx_ff[j] += eps;\n",
    "            grad_f = self.gradiente(dx_ff)\n",
    "            grad_b = self.gradiente(dx_bb)\n",
    "            for i in range(j+1):\n",
    "                # Segunda derivada\n",
    "                hess[i, j] = (grad_f[i] - grad_b[i]) / (2 * eps)\n",
    "                hess[j, i] = (grad_f[i] - grad_b[i]) / (2 * eps)\n",
    "        return hess\n",
    "    \n",
    "    # Función que aproxima el mínimo con el método de Newton (alpha estática)\n",
    "    def Newton(self, xk, alpha, eps = 1e-6, max_iter = 1000): \n",
    "        for k in range(max_iter):\n",
    "            grad = self.gradiente(xk)\n",
    "            if(np.linalg.norm(grad) < eps): break\n",
    "            hess = self.hessiana(xk)\n",
    "            try: # Si la hessiana no es positiva definida causa error\n",
    "                pk = np.linalg.solve(hess, -grad)\n",
    "            except np.linalg.LinAlgError as err:\n",
    "                print(\"La hessiana no es positiva definida\")\n",
    "                break\n",
    "            xk = xk + alpha * pk\n",
    "        return xk\n",
    "    \n",
    "    # Función que calcula el tamaño del paso, haciendo que se cumpla la función de Wolfe\n",
    "    def BLS(self, xk, alpha_0, pk, c):\n",
    "        alpha = alpha_0\n",
    "        while self.f(xk + alpha * pk) > self.f(xk) + c * alpha * self.gradiente(xk) @ pk:\n",
    "            p = random()\n",
    "            alpha = p * alpha\n",
    "        return alpha\n",
    "        \n",
    "    def LSN(self, xk):\n",
    "        for k in range(self.max_iter):\n",
    "            grad = self.gradiente(xk)\n",
    "            if(np.linalg.norm(grad) < self.tol): break\n",
    "            Bk = self.hessiana(xk)\n",
    "            try:\n",
    "                pk = np.linalg.solve(Bk, - grad)\n",
    "            except np.linalg.LinAlgError as err:\n",
    "                print(\"La hessiana no es positiva definida\")\n",
    "                break\n",
    "            xk = xk + self.BLS(xk, 1, pk, 0.5) * pk\n",
    "        return xk\n",
    "\n",
    "    #Función que vuelve una matriz en positiva definida\n",
    "    def CAMI(self,A):\n",
    "        beta = 0.001\n",
    "        n = len(A)\n",
    "        if min(np.diag(A)) > 0:\n",
    "            t = 0\n",
    "        else:\n",
    "            t = -min(np.diag(A)) + beta\n",
    "        for k in range(self.max_iter):\n",
    "            try:\n",
    "                np.linalg.cholesky(A + t * np.identity(n))\n",
    "            except np.linalg.LinAlgError as err: # Si no es pos. definida, actualizamos\n",
    "                t = max(2 * t, beta)\n",
    "            else:\n",
    "                break\n",
    "        return A + t * np.identity(n)\n",
    "    \n",
    "    # Función que aproxima el mínimo con el método de Newton (alpha varía) y con modificación a la Hessiana\n",
    "    def LSNM(self, xk):\n",
    "        for k in range(self.max_iter):\n",
    "            grad = self.gradiente(xk)\n",
    "            if(np.linalg.norm(grad) < self.tol): break\n",
    "            Bk = self.hessiana(xk)\n",
    "            try:\n",
    "                np.linalg.cholesky(Bk)\n",
    "            except np.linalg.LinAlgError as err:\n",
    "                Bk = self.CAMI(Bk)   \n",
    "            pk = np.linalg.solve(Bk, - grad)\n",
    "            xk = xk + self.BLS(xk, 1, pk, 0.5) * pk\n",
    "        return xk\n",
    "\n",
    "    #Función que calcula el mínimo por Broyden, Fletcher, Goldfarb y Shanno.\n",
    "    def BFGS(self, xk, Hk):\n",
    "        I = np.identity(len(xk))\n",
    "        for k in range(self.max_iter):\n",
    "            grad = self.gradiente(xk)\n",
    "            if(np.linalg.norm(grad) < self.tol): break\n",
    "            #Dirección de Descenso\n",
    "            pk = -Hk @ grad\n",
    "            #Calculamos el tamaño del paso\n",
    "            alpha = self.BLS(xk, 1, pk, 0.5)\n",
    "            #Nueva x\n",
    "            sk = alpha*pk; sk_t = sk.transpose()\n",
    "            xk = xk + sk\n",
    "            #Calculamos yk y phi_k\n",
    "            yk = self.gradiente(xk) - grad; yk_t = yk.transpose();\n",
    "            if(yk_t@sk == 0): break\n",
    "            phi_k = 1.0/(yk_t@sk)\n",
    "            #Calculamos la nueva hessiana inversa\n",
    "            Hk = (I - phi_k*sk@yk_t)@Hk@(I - phi_k*sk@yk_t) + phi_k * sk @ sk_t\n",
    "        return xk\n",
    "    \n",
    "    def optimiza(self, metodo):\n",
    "        if metodo == \"Newton\":\n",
    "            res = self.Newton(self.x0,0.1)\n",
    "        elif metodo == \"LSN\":\n",
    "            res = self.LSN(self.x0)\n",
    "        elif metodo == \"LSNM\":\n",
    "            res = self.LSNM(self.x0)\n",
    "        elif metodo == \"BFGS\":\n",
    "            res = self.BFGS(self.x0, np.identity(len(self.x0)))\n",
    "        else:\n",
    "            print(\"Método no encontrado\")\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rosenbrock(x, a = 1, b = 100):\n",
    "    return (a - x[0])**2 + b * (x[1] - x[0]**2)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prueba de aproximaciones\n",
      "Newton: [0.99999951 0.99999903]\n",
      "LSN: [0.99954927 0.99909859]\n",
      "LSNM [1.0004675  1.00093506]\n",
      "BFGS [1.00083624 1.00167563]\n"
     ]
    }
   ],
   "source": [
    "print(\"Prueba de aproximaciones\")\n",
    "opt = Optimizador(f = Rosenbrock,x0 = np.array([0.9, 1.1]))\n",
    "print(\"Newton:\",opt.optimiza(\"Newton\"))\n",
    "print(\"LSN:\",opt.optimiza(\"LSN\"))\n",
    "print(\"LSNM\", opt.optimiza(\"LSNM\"))\n",
    "print(\"BFGS\", opt.optimiza(\"BFGS\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
