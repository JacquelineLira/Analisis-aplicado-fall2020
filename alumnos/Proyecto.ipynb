{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "\n",
    "#Función para ejemplos\n",
    "def g(x0):\n",
    "    return sum(x0**2+2*x0-5)\n",
    "\n",
    "x0 = np.array([4,2,5,2])\n",
    "\n",
    "\n",
    "# # Tarea 1\n",
    "# 10.9.20 \n",
    "# \n",
    "# 173199\n",
    "\n",
    "# ### Gradiente\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "def grad(f, x0):\n",
    "    n = x0.size\n",
    "    eps = 0.00001\n",
    "    res = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        aux = np.zeros(n)\n",
    "        aux[i] = eps\n",
    "        x1 = x0 + aux\n",
    "        res[i] = (f(x1) - f(x0))/eps        \n",
    "    return res\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "grad(g,x0)\n",
    "\n",
    "\n",
    "# ### Hessiana\n",
    "# En hess1 intenté hacer lo del siguiente link, pero no me quedaba: http://www2.math.umd.edu/~dlevy/classes/amsc466/lecture-notes/differentiation-chap.pdf.\n",
    "# \n",
    "# En hess, usé diferenciación como en: https://neos-guide.org/content/difference-approximations#:~:text=One%20method%20for%20approximating%20the,evaluated%20at%20two%20nearby%20points.\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "def hess1(f,x0):\n",
    "    n = x0.size\n",
    "    eps = 0.00001\n",
    "    res = np.zeros([n,n])\n",
    "    for i in range(n):\n",
    "        for j in range(i+1): #porque es simétrica, y entonces mejor res[j][i] = res[i][j]\n",
    "            aux1 = np.zeros(n)\n",
    "            aux2 = np.zeros(n)\n",
    "            #aux1 = aux2 no jala en numpy; mejor np.copy\n",
    "            aux1[i] = eps\n",
    "            aux2[j] = eps\n",
    "            xij = x0 + aux1 + aux2\n",
    "            xi = x0 + aux1\n",
    "            xj = x0 + aux2\n",
    "            res[i][j] = (f(xij) - f(xi) - f(xj) + f(x0))/(eps**2)\n",
    "            res[j][i] = res[i][j]\n",
    "    return res\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "hess1(g,x0)\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "def hess(f,x0):\n",
    "    n = x0.size\n",
    "    eps = 0.00001\n",
    "    res = np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        aux = np.zeros(n)\n",
    "        aux[i] = eps\n",
    "        x1 = x0 + aux\n",
    "        res[:,i] = (grad(f,x1)-grad(f,x0))/eps\n",
    "    return res\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "hess(g,x0)\n",
    "\n",
    "\n",
    "# ### Condiciones de optimalidad\n",
    "# Queremos checar los second orden necessary conditions, es decir,\n",
    "# \n",
    "# $∇f(x^*) = 0$\n",
    "# \n",
    "# $∇^2f(x^*)$ semidefinida positiva (para esto, lo más fácil es usar los eigenvalores)\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "def condiciones_optimalidad(f,x0):\n",
    "    res = \"\"\n",
    "    if np.all(grad(f,x0) == 0):\n",
    "        res += \"Cumple con tener gradiente 0. \"\n",
    "    else:\n",
    "        res += \"No cumple con tener gradiente 0. \"\n",
    "    \n",
    "    eigs = np.linalg.eigvals(hess(f,x0))\n",
    "    if np.all(eigs > 0):\n",
    "        res += \"Cumple con tener Hessiana semidefinida.\"\n",
    "    else:\n",
    "        res += \"No cumple con tener Hessiana semidefinida.\"\n",
    "        \n",
    "    return res\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "condiciones_optimalidad(g,x0)\n",
    "\n",
    "\n",
    "# ### Función de aproximación\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "def mk (f,x0,p):\n",
    "    H = hess(f,x0)\n",
    "    G = grad(f,x0)\n",
    "    aux = np.dot(p.T,H)\n",
    "    return f(x0) + np.dot(p.T,G) + 0.5*np.dot(aux,p)\n",
    "\n",
    "\n",
    "# ### Ejemplo\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "print(grad(g,x0))\n",
    "print(hess(g,x0))\n",
    "p = np.array([1,2,3,4])\n",
    "p = p.T\n",
    "print(condiciones_optimalidad(g,x0))\n",
    "print(mk(g,x0,p))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Tarea 2\n",
    "# 11.10.20\n",
    "# \n",
    "# 173199 (Alejandro Chávez) en equipo con: 162136 (Manuel García), 149427 (Héctor Vela), 174144 (Karla Alva)\n",
    "\n",
    "# ### Para encontrar $\\alpha$ (Algoritmo 3.1)\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "def alfa(f,x0,p):\n",
    "    a_gorro = 1\n",
    "    ro = 0.8\n",
    "    c = 0.0001\n",
    "    alpha = a_gorro\n",
    "    while f(x0+alpha*p) > f(x0)+c*alpha*(grad(f,x0).T).dot(p):\n",
    "        alpha = ro*alpha\n",
    "        \n",
    "    return alpha\n",
    "\n",
    "\n",
    "# ### Cholesky with Added Multiple of the Identity (Algoritmo 3.3)\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "def calcula_gamma(A):\n",
    "    k = 100\n",
    "    n = A.shape[0]\n",
    "    beta = 0.001\n",
    "    \n",
    "    diagonal = np.diagonal(A)\n",
    "    minimo = np.amin(diagonal)\n",
    "    if minimo > 0:\n",
    "        gamma = 0\n",
    "    else:\n",
    "        gamma = -minimo + beta\n",
    "    #end if\n",
    "    \n",
    "    for i in range(k):     \n",
    "        try:\n",
    "            np.linalg.cholesky(A+gamma*np.identity(n))\n",
    "        except np.linalg.LinAlgError:\n",
    "            gamma = max([2*gamma, beta])\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "        \n",
    "    return gamma\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "A = np.matrix([[-8,-1,0], [-1,2,-1],[0,-1,2]])\n",
    "A\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "print(calcula_gamma(A))\n",
    "\n",
    "\n",
    "# ### Método de Newton con modificación a la Hessiana (Algoritmo 3.2)\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "#nuestro primer intento, sin saber de la existencia del algoritmo 3.3\n",
    "def newton_mod1(f,x0):\n",
    "    k = 1000\n",
    "    gamma = 0.01\n",
    "    \n",
    "    B0 = hess(f,x0)\n",
    "    eigs = np.linalg.eigvals(B0)  \n",
    "    \n",
    "    for i in range(k):\n",
    "        while not np.all(eigs > 0): #inicia while\n",
    "            B0 = hess(f,x0)+gamma\n",
    "            gamma = gamma + 0.01\n",
    "            eigs = np.linalg.eigvals(B0)\n",
    "        #terminó while\n",
    "        \n",
    "        p0 = -np.linalg.inv(B0).dot(grad(f,x0))        \n",
    "        alpha = alfa(f,x0,p0)\n",
    "        x0 = x0+alpha*p0\n",
    "        #print(f(x0)) (para ver si la función realmente decrece)\n",
    "        \n",
    "    return x0\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "#nuestro segundo intento, sabiendo de la existencia del 3.3\n",
    "def newton_mod(f,x0):\n",
    "    k = 1000\n",
    "    n = x0.size\n",
    "    \n",
    "    for i in range(k):\n",
    "        gamma = calcula_gamma(hess(f,x0))\n",
    "        B0 = hess(f,x0)+gamma*np.identity(n)\n",
    "        \n",
    "        p0 = -np.linalg.inv(B0).dot(grad(f,x0))        \n",
    "        alpha = alfa(f,x0,p0)\n",
    "        x0 = x0+alpha*p0\n",
    "        #print(f(x0)) (para ver si la función realmente decrece)\n",
    "        \n",
    "    return x0\n",
    "\n",
    "\n",
    "# ### Rosenbrock\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "def rosenbrock(x0):\n",
    "    a = 1\n",
    "    b = 100\n",
    "    return (a-x0[0])**2+b*(x0[1]-x0[0]**2)**2\n",
    "\n",
    "\n",
    "x0 = np.array([1,1])\n",
    "rosenbrock(x0) #como es el mínimo debería de dar cero\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "x0 = np.array([1,1])\n",
    "p = np.array([1,2])\n",
    "p = p.T\n",
    "alpha = alfa(rosenbrock,x0,p)\n",
    "print (alpha)\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "#primer intento\n",
    "x0 = np.array([22,50])\n",
    "respuesta = newton_mod1(rosenbrock,x0)\n",
    "print(respuesta)\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "#segundo intento\n",
    "x0 = np.array([22,50])\n",
    "respuesta = newton_mod(rosenbrock,x0)\n",
    "print(respuesta)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
